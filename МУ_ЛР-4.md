# –ø–æ—à–∞–≥–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –õ–†4 "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
(MLflow)" –†–∞–±–æ—Ç–∞–µ–º –ø–æ –ø—Ä–µ–∂–Ω–µ–º—É –Ω–∞ —à–∞–±–ª–æ–Ω–µ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è [https://github.com/Irina-64/mlops-flight-delay](https://github.com/Irina-64/mlops-flight-delay)


**–¶–µ–ª—å:** –Ω–∞–ø–∏—Å–∞—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Å–∫—Ä–∏–ø—Ç, –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –≤ MLflow.
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è: src/train.py, data/processed/processed.csv, MLflow —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.

**–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** 4-5 —á–∞—Å–æ–≤

**–ò—Å—Ö–æ–¥–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª:** –í—ã–ø–æ–ª–Ω–µ–Ω–∞ –õ–†3 (DVC –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö)

***

## üìã –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —Ä–∞–±–æ—Ç–µ

### –®–∞–≥ 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è —Å MLflow

```bash
# –ü–µ—Ä–µ—Ö–æ–¥–∏–º –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞
cd mlops-flight-delay

# –û–±–Ω–æ–≤–ª—è–µ–º requirements.txt
cat >> requirements.txt << EOF
mlflow>=2.7.1
scikit-learn>=1.3.0
xgboost>=1.7.6
hyperopt>=0.2.7
matplotlib>=3.7.2
seaborn>=0.12.2
shap>=0.42.1
EOF

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install -r requirements.txt
```


### –®–∞–≥ 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MLflow

```bash
# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è MLflow
mkdir -p mlruns experiments

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º MLflow tracking server (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ)
mlflow ui --host 0.0.0.0 --port 5000

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ MLflow UI –¥–æ—Å—Ç—É–ø–µ–Ω: http://localhost:5000
```

## –ß–∞—Å—Ç—å 1. –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞ –æ–±—É—á–µ–Ω–∏—è


### –®–∞–≥ 3. –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤

–û–±–Ω–æ–≤–∏—Ç–µ params.yaml:

```text
data:
  raw: "data/raw/flights.csv"
  processed: "data/processed/flights_processed.csv"
  split_dir: "data/split"

preprocess:
  handle_missing: true
  remove_outliers: true
  test_size: 0.2
  random_state: 42

experiments:
  experiment_name: "flight_delay_prediction"
  tracking_uri: "http://localhost:5000"
  
models:
  random_forest:
    n_estimators: [50, 100, 200]
    max_depth: [5, 10, 15, null]
    min_samples_split: [2, 5, 10]
    random_state: 42
    
  xgboost:
    n_estimators: [50, 100, 200]
    max_depth: [3, 6, 10]
    learning_rate: [0.01, 0.1, 0.2]
    random_state: 42
    
  logistic_regression:
    C: [0.1, 1.0, 10.0]
    max_iter: [1000]
    random_state: 42

hyperparameter_tuning:
  max_evals: 20
  cv_folds: 3
  scoring: "f1"
```


### –®–∞–≥ 4. –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞ –æ–±—É—á–µ–Ω–∏—è

–°–æ–∑–¥–∞–π—Ç–µ src/train.py:

```python
import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
import mlflow.xgboost
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, confusion_matrix, 
                           classification_report)
import xgboost as xgb
import yaml
import argparse
import joblib
import os
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

def load_config(config_path='params.yaml'):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def load_data(config):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    split_dir = config['data']['split_dir']
    
    X_train = pd.read_csv(f'{split_dir}/X_train.csv')
    X_test = pd.read_csv(f'{split_dir}/X_test.csv')
    y_train = pd.read_csv(f'{split_dir}/y_train.csv').values.ravel()
    y_test = pd.read_csv(f'{split_dir}/y_test.csv').values.ravel()
    
    return X_train, X_test, y_train, y_test

def calculate_metrics(y_true, y_pred, y_pred_proba=None):
    """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, average='weighted'),
        'recall': recall_score(y_true, y_pred, average='weighted'),
        'f1_score': f1_score(y_true, y_pred, average='weighted')
    }
    
    if y_pred_proba is not None:
        # –î–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if len(np.unique(y_true)) == 2:
            metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba[:, 1])
        else:
            metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba, multi_class='ovr')
    
    return metrics

def plot_confusion_matrix(y_true, y_pred, model_name):
    """–°–æ–∑–¥–∞–Ω–∏–µ confusion matrix"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {model_name}')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
    plot_path = f'reports/confusion_matrix_{model_name.lower()}.png'
    os.makedirs('reports', exist_ok=True)
    plt.savefig(plot_path)
    plt.close()
    
    return plot_path

def plot_feature_importance(model, feature_names, model_name):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1][:15]  # –¢–æ–ø-15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        plt.figure(figsize=(12, 8))
        plt.title(f"Feature Importance - {model_name}")
        plt.bar(range(len(indices)), importances[indices])
        plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=45)
        plt.tight_layout()
        
        plot_path = f'reports/feature_importance_{model_name.lower()}.png'
        plt.savefig(plot_path)
        plt.close()
        
        return plot_path
    return None

def train_model(model_type, model_params, X_train, y_train, X_test, y_test, feature_names):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤ MLflow"""
    
    with mlflow.start_run(run_name=f"{model_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        mlflow.log_params(model_params)
        mlflow.set_tag("model_type", model_type)
        mlflow.set_tag("dataset", "flight_delays")
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        if model_type == "random_forest":
            model = RandomForestClassifier(**model_params)
            mlflow.sklearn.autolog()
        elif model_type == "xgboost":
            model = xgb.XGBClassifier(**model_params)
            mlflow.xgboost.autolog()
        elif model_type == "logistic_regression":
            model = LogisticRegression(**model_params)
            mlflow.sklearn.autolog()
        else:
            raise ValueError(f"Unknown model type: {model_type}")
        
        # –û–±—É—á–µ–Ω–∏–µ
        print(f"Training {model_type} model...")
        model.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è ROC-AUC
        if hasattr(model, 'predict_proba'):
            y_pred_proba_train = model.predict_proba(X_train)
            y_pred_proba_test = model.predict_proba(X_test)
        else:
            y_pred_proba_train = None
            y_pred_proba_test = None
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        train_metrics = calculate_metrics(y_train, y_pred_train, y_pred_proba_train)
        test_metrics = calculate_metrics(y_test, y_pred_test, y_pred_proba_test)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
        for metric, value in train_metrics.items():
            mlflow.log_metric(f"train_{metric}", value)
        
        for metric, value in test_metrics.items():
            mlflow.log_metric(f"test_{metric}", value)
        
        # –õ–æ–≥–∏—Ä—É–µ–º overfitting –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä
        mlflow.log_metric("overfitting_score", 
                         train_metrics['f1_score'] - test_metrics['f1_score'])
        
        # –°–æ–∑–¥–∞–µ–º –∏ –ª–æ–≥–∏—Ä—É–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
        cm_path = plot_confusion_matrix(y_test, y_pred_test, model_type)
        mlflow.log_artifact(cm_path)
        
        fi_path = plot_feature_importance(model, feature_names, model_type)
        if fi_path:
            mlflow.log_artifact(fi_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        model_path = f"models/{model_type}_model.pkl"
        os.makedirs("models", exist_ok=True)
        joblib.dump(model, model_path)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –≤ MLflow
        if model_type in ["random_forest", "logistic_regression"]:
            mlflow.sklearn.log_model(model, "model")
        elif model_type == "xgboost":
            mlflow.xgboost.log_model(model, "model")
        
        mlflow.log_artifact(model_path)
        
        # –õ–æ–≥–∏—Ä—É–µ–º classification report
        class_report = classification_report(y_test, y_pred_test)
        with open(f"reports/classification_report_{model_type}.txt", "w") as f:
            f.write(class_report)
        mlflow.log_artifact(f"reports/classification_report_{model_type}.txt")
        
        print(f"\n{model_type.upper()} Results:")
        print(f"Train F1: {train_metrics['f1_score']:.4f}")
        print(f"Test F1: {test_metrics['f1_score']:.4f}")
        print(f"Test Accuracy: {test_metrics['accuracy']:.4f}")
        if 'roc_auc' in test_metrics:
            print(f"Test ROC-AUC: {test_metrics['roc_auc']:.4f}")
        
        return model, test_metrics['f1_score']

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="params.yaml", help="Config file path")
    parser.add_argument("--model", help="Specific model to train (random_forest, xgboost, logistic_regression)")
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = load_config(args.config)
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º MLflow
    if config['experiments']['tracking_uri']:
        mlflow.set_tracking_uri(config['experiments']['tracking_uri'])
    
    experiment_name = config['experiments']['experiment_name']
    experiment = mlflow.get_experiment_by_name(experiment_name)
    if experiment is None:
        experiment_id = mlflow.create_experiment(experiment_name)
    else:
        experiment_id = experiment.experiment_id
    
    mlflow.set_experiment(experiment_name)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print("Loading data...")
    X_train, X_test, y_train, y_test = load_data(config)
    feature_names = X_train.columns.tolist()
    
    print(f"Train set shape: {X_train.shape}")
    print(f"Test set shape: {X_test.shape}")
    print(f"Target distribution: {np.bincount(y_train)}")
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
    results = {}
    models_to_train = [args.model] if args.model else config['models'].keys()
    
    for model_type in models_to_train:
        if model_type not in config['models']:
            print(f"Warning: {model_type} not found in config")
            continue
        
        model_params = config['models'][model_type]
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ —Å–ø–∏—Å–∫–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        base_params = {}
        for param, value in model_params.items():
            if isinstance(value, list):
                base_params[param] = value[0]
            else:
                base_params[param] = value
        
        model, score = train_model(
            model_type, base_params, 
            X_train, y_train, X_test, y_test, feature_names
        )
        results[model_type] = score
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "="*50)
    print("COMPARISON OF MODELS:")
    print("="*50)
    for model_type, score in sorted(results.items(), key=lambda x: x[1], reverse=True):
        print(f"{model_type:20}: {score:.4f}")

if __name__ == "__main__":
    main()

```


***

## üîß –ß–∞—Å—Ç—å 2. –ó–∞–ø—É—Å–∫ –±–∞–∑–æ–≤—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤

### –®–∞–≥ 5. –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è

```bash
# –£–±–µ–¥–∏–º—Å—è —á—Ç–æ MLflow UI –∑–∞–ø—É—â–µ–Ω (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ)
mlflow ui --host 0.0.0.0 --port 5000

# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
python src/train.py

# –ò–ª–∏ –æ–±—É—á–∞–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å
python src/train.py --model random_forest

```
**
–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –í MLflow UI (http://localhost:5000) –¥–æ–ª–∂–Ω—ã –ø–æ—è–≤–∏—Ç—å—Å—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏, –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞–º–∏.
**

### –®–∞–≥ 6. –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```bash
# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
ls -la models/
ls -la reports/

# –û—Ç–∫—Ä—ã–≤–∞–µ–º MLflow UI –∏ –∏–∑—É—á–∞–µ–º:
# 1. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∂–¥–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
# 2. –ú–µ—Ç—Ä–∏–∫–∏ (accuracy, f1_score, roc_auc)
# 3. –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã (–≥—Ä–∞—Ñ–∏–∫–∏, –º–æ–¥–µ–ª–∏, –æ—Ç—á–µ—Ç—ã)

```
## –ß–∞—Å—Ç—å 3. –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### –®–∞–≥ 7. –æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞ –¥–ª—è hyperparameter tuning

–°–æ–∑–¥–∞–π—Ç–µ src/hyperopt_tune.py:

```python
import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
import mlflow.xgboost
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import f1_score
import xgboost as xgb
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval
import yaml
import argparse
import pickle
from datetime import datetime

def load_config(config_path='params.yaml'):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def load_data(config):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    split_dir = config['data']['split_dir']
    
    X_train = pd.read_csv(f'{split_dir}/X_train.csv')
    X_test = pd.read_csv(f'{split_dir}/X_test.csv')
    y_train = pd.read_csv(f'{split_dir}/y_train.csv').values.ravel()
    y_test = pd.read_csv(f'{split_dir}/y_test.csv').values.ravel()
    
    return X_train, X_test, y_train, y_test

def objective_random_forest(params, X_train, y_train, cv_folds, scoring):
    """Objective function for Random Forest optimization"""
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –Ω—É–∂–Ω—ã–µ —Ç–∏–ø—ã
    params = {
        'n_estimators': int(params['n_estimators']),
        'max_depth': int(params['max_depth']) if params['max_depth'] != 0 else None,
        'min_samples_split': int(params['min_samples_split']),
        'random_state': 42
    }
    
    model = RandomForestClassifier(**params)
    
    # Cross-validation
    scores = cross_val_score(model, X_train, y_train, cv=cv_folds, scoring=scoring)
    
    # MLflow logging
    with mlflow.start_run(nested=True):
        mlflow.log_params(params)
        mlflow.log_metric('cv_score_mean', scores.mean())
        mlflow.log_metric('cv_score_std', scores.std())
        mlflow.set_tag("optimization", "hyperopt")
    
    # Hyperopt minimizes, so return negative score
    return {'loss': -scores.mean(), 'status': STATUS_OK}

def objective_xgboost(params, X_train, y_train, cv_folds, scoring):
    """Objective function for XGBoost optimization"""
    
    params = {
        'n_estimators': int(params['n_estimators']),
        'max_depth': int(params['max_depth']),
        'learning_rate': params['learning_rate'],
        'random_state': 42,
        'eval_metric': 'logloss'
    }
    
    model = xgb.XGBClassifier(**params)
    
    scores = cross_val_score(model, X_train, y_train, cv=cv_folds, scoring=scoring)
    
    with mlflow.start_run(nested=True):
        mlflow.log_params(params)
        mlflow.log_metric('cv_score_mean', scores.mean())
        mlflow.log_metric('cv_score_std', scores.std())
        mlflow.set_tag("optimization", "hyperopt")
    
    return {'loss': -scores.mean(), 'status': STATUS_OK}

def objective_logistic_regression(params, X_train, y_train, cv_folds, scoring):
    """Objective function for Logistic Regression optimization"""
    
    params = {
        'C': params['C'],
        'max_iter': int(params['max_iter']),
        'random_state': 42
    }
    
    model = LogisticRegression(**params)
    
    scores = cross_val_score(model, X_train, y_train, cv=cv_folds, scoring=scoring)
    
    with mlflow.start_run(nested=True):
        mlflow.log_params(params)
        mlflow.log_metric('cv_score_mean', scores.mean())
        mlflow.log_metric('cv_score_std', scores.std())
        mlflow.set_tag("optimization", "hyperopt")
    
    return {'loss': -scores.mean(), 'status': STATUS_OK}

def optimize_model(model_type, X_train, y_train, config):
    """Hyperparameter optimization for specified model"""
    
    hyperopt_config = config['hyperparameter_tuning']
    max_evals = hyperopt_config['max_evals']
    cv_folds = hyperopt_config['cv_folds']
    scoring = hyperopt_config['scoring']
    
    # Define search spaces
    if model_type == 'random_forest':
        space = {
            'n_estimators': hp.choice('n_estimators', [50, 100, 200, 300]),
            'max_depth': hp.choice('max_depth', [0, 5, 10, 15, 20]),  # 0 means None
            'min_samples_split': hp.choice('min_samples_split', [2, 5, 10, 15])
        }
        objective = lambda params: objective_random_forest(params, X_train, y_train, cv_folds, scoring)
        
    elif model_type == 'xgboost':
        space = {
            'n_estimators': hp.choice('n_estimators', [50, 100, 200, 300]),
            'max_depth': hp.choice('max_depth', [3, 6, 9, 12]),
            'learning_rate': hp.choice('learning_rate', [0.01, 0.05, 0.1, 0.2])
        }
        objective = lambda params: objective_xgboost(params, X_train, y_train, cv_folds, scoring)
        
    elif model_type == 'logistic_regression':
        space = {
            'C': hp.choice('C', [0.01, 0.1, 1.0, 10.0, 100.0]),
            'max_iter': hp.choice('max_iter', [500, 1000, 2000])
        }
        objective = lambda params: objective_logistic_regression(params, X_train, y_train, cv_folds, scoring)
    
    else:
        raise ValueError(f"Unknown model type: {model_type}")
    
    # Run optimization
    print(f"Starting hyperparameter optimization for {model_type}")
    print(f"Max evaluations: {max_evals}")
    
    with mlflow.start_run(run_name=f"hyperopt_{model_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
        mlflow.set_tag("optimization_type", "hyperopt")
        mlflow.set_tag("model_type", model_type)
        mlflow.log_param("max_evals", max_evals)
        mlflow.log_param("cv_folds", cv_folds)
        mlflow.log_param("scoring", scoring)
        
        trials = Trials()
        
        best = fmin(
            fn=objective,
            space=space,
            algo=tpe.suggest,
            max_evals=max_evals,
            trials=trials
        )
        
        # Get best parameters
        best_params = space_eval(space, best)
        
        # Log best parameters and score
        best_score = -trials.best_trial['result']['loss']
        mlflow.log_params({f"best_{k}": v for k, v in best_params.items()})
        mlflow.log_metric("best_cv_score", best_score)
        
        # Save trials for analysis
        with open(f"experiments/hyperopt_trials_{model_type}.pkl", "wb") as f:
            pickle.dump(trials, f)
        
        print(f"Best parameters for {model_type}: {best_params}")
        print(f"Best CV score: {best_score:.4f}")
        
        return best_params, best_score

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="params.yaml", help="Config file path")
    parser.add_argument("--model", required=True, 
                       choices=['random_forest', 'xgboost', 'logistic_regression'],
                       help="Model to optimize")
    args = parser.parse_args()
    
    config = load_config(args.config)
    
    # Setup MLflow
    if config['experiments']['tracking_uri']:
        mlflow.set_tracking_uri(config['experiments']['tracking_uri'])
    
    experiment_name = f"{config['experiments']['experiment_name']}_hyperopt"
    experiment = mlflow.get_experiment_by_name(experiment_name)
    if experiment is None:
        mlflow.create_experiment(experiment_name)
    mlflow.set_experiment(experiment_name)
    
    # Load data
    X_train, X_test, y_train, y_test = load_data(config)
    
    # Run optimization
    best_params, best_score = optimize_model(args.model, X_train, y_train, config)
    
    print(f"\nOptimization completed!")
    print(f"Best {args.model} parameters: {best_params}")
    print(f"Best CV score: {best_score:.4f}")

if __name__ == "__main__":
    main()

```

### –®–∞–≥ 8. –ó–∞–ø—É—Å–∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

```bash
# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
mkdir -p experiments

# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –¥–ª—è Random Forest
python src/hyperopt_tune.py --model random_forest

# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –¥–ª—è XGBoost
python src/hyperopt_tune.py --model xgboost

# –ó–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –¥–ª—è Logistic Regression
python src/hyperopt_tune.py --model logistic_regression

```

## üìä –ß–∞—Å—Ç—å 4. –û–±—É—á–µ–Ω–∏–µ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏


### –®–∞–≥ 9. –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

–°–æ–∑–¥–∞–π—Ç–µ src/train_best_model.py:

```python
import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
import mlflow.xgboost
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, confusion_matrix)
import xgboost as xgb
import yaml
import argparse
import joblib
import os
import pickle
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import shap

def load_config(config_path='params.yaml'):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def load_data(config):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    split_dir = config['data']['split_dir']
    
    X_train = pd.read_csv(f'{split_dir}/X_train.csv')
    X_test = pd.read_csv(f'{split_dir}/X_test.csv')
    y_train = pd.read_csv(f'{split_dir}/y_train.csv').values.ravel()
    y_test = pd.read_csv(f'{split_dir}/y_test.csv').values.ravel()
    
    return X_train, X_test, y_train, y_test

def load_best_params_from_mlflow(experiment_name, model_type):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ MLflow"""
    
    experiment = mlflow.get_experiment_by_name(experiment_name)
    if experiment is None:
        print(f"Experiment {experiment_name} not found")
        return None
    
    runs = mlflow.search_runs(
        experiment_ids=[experiment.experiment_id],
        filter_string=f"tags.model_type = '{model_type}'",
        order_by=["metrics.best_cv_score DESC"],
        max_results=1
    )
    
    if runs.empty:
        print(f"No runs found for {model_type}")
        return None
    
    best_run = runs.iloc[0]
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    best_params = {}
    for col in best_run.index:
        if col.startswith(f'params.best_'):
            param_name = col.replace(f'params.best_', '')
            param_value = best_run[col]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–∏–ø—ã
            if param_name in ['n_estimators', 'max_depth', 'min_samples_split', 'max_iter']:
                if param_value == 'None':
                    best_params[param_name] = None
                else:
                    best_params[param_name] = int(float(param_value))
            elif param_name in ['learning_rate', 'C']:
                best_params[param_name] = float(param_value)
            else:
                best_params[param_name] = param_value
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    best_params['random_state'] = 42
    if model_type == 'xgboost':
        best_params['eval_metric'] = 'logloss'
    
    print(f"Best parameters for {model_type}: {best_params}")
    return best_params

def create_shap_plots(model, X_test, model_name):
    """–°–æ–∑–¥–∞–Ω–∏–µ SHAP –≥—Ä–∞—Ñ–∏–∫–æ–≤"""
    try:
        # –°–æ–∑–¥–∞–µ–º explainer
        if hasattr(model, 'predict_proba'):
            explainer = shap.Explainer(model, X_test.sample(min(100, len(X_test))))
        else:
            explainer = shap.Explainer(model.predict, X_test.sample(min(100, len(X_test))))
        
        # –í—ã—á–∏—Å–ª—è–µ–º SHAP values –¥–ª—è –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏
        X_sample = X_test.sample(min(50, len(X_test)))
        shap_values = explainer(X_sample)
        
        # Summary plot
        plt.figure(figsize=(10, 8))
        if hasattr(shap_values, 'values') and shap_values.values.ndim > 2:
            shap.summary_plot(shap_values.values[:, :, 1], X_sample, show=False)
        else:
            shap.summary_plot(shap_values, X_sample, show=False)
        plt.title(f'SHAP Summary Plot - {model_name}')
        
        shap_path = f'reports/shap_summary_{model_name.lower()}.png'
        plt.savefig(shap_path, bbox_inches='tight')
        plt.close()
        
        return shap_path
        
    except Exception as e:
        print(f"Could not create SHAP plots: {e}")
        return None

def train_final_model(model_type, best_params, X_train, y_train, X_test, y_test):
    """–û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    
    run_name = f"final_{model_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    with mlflow.start_run(run_name=run_name):
        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        mlflow.log_params(best_params)
        mlflow.set_tag("model_type", model_type)
        mlflow.set_tag("model_stage", "final")
        mlflow.set_tag("dataset", "flight_delays")
        
        # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        if model_type == "random_forest":
            model = RandomForestClassifier(**best_params)
        elif model_type == "xgboost":
            model = xgb.XGBClassifier(**best_params)
        elif model_type == "logistic_regression":
            model = LogisticRegression(**best_params)
        
        print(f"Training final {model_type} model...")
        model.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        if hasattr(model, 'predict_proba'):
            y_pred_proba_test = model.predict_proba(X_test)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        train_metrics = {
            'accuracy': accuracy_score(y_train, y_pred_train),
            'precision': precision_score(y_train, y_pred_train, average='weighted'),
            'recall': recall_score(y_train, y_pred_train, average='weighted'),
            'f1_score': f1_score(y_train, y_pred_train, average='weighted')
        }
        
        test_metrics = {
            'accuracy': accuracy_score(y_test, y_pred_test),
            'precision': precision_score(y_test, y_pred_test, average='weighted'),
            'recall': recall_score(y_test, y_pred_test, average='weighted'),
            'f1_score': f1_score(y_test, y_pred_test, average='weighted')
        }
        
        if hasattr(model, 'predict_proba'):
            test_metrics['roc_auc'] = roc_auc_score(y_test, y_pred_proba_test[:, 1])
        
        # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
        for metric, value in train_metrics.items():
            mlflow.log_metric(f"final_train_{metric}", value)
        
        for metric, value in test_metrics.items():
            mlflow.log_metric(f"final_test_{metric}", value)
        
        mlflow.log_metric("final_overfitting", 
                         train_metrics['f1_score'] - test_metrics['f1_score'])
        
        # Confusion Matrix
        cm = confusion_matrix(y_test, y_pred_test)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title(f'Final Model Confusion Matrix - {model_type}')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        
        cm_path = f'reports/final_confusion_matrix_{model_type}.png'
        os.makedirs('reports', exist_ok=True)
        plt.savefig(cm_path)
        plt.close()
        mlflow.log_artifact(cm_path)
        
        # Feature Importance
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
            feature_names = X_train.columns.tolist()
            indices = np.argsort(importances)[::-1][:15]
            
            plt.figure(figsize=(12, 8))
            plt.title(f"Final Model Feature Importance - {model_type}")
            plt.bar(range(len(indices)), importances[indices])
            plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=45)
            plt.tight_layout()
            
            fi_path = f'reports/final_feature_importance_{model_type}.png'
            plt.savefig(fi_path)
            plt.close()
            mlflow.log_artifact(fi_path)
        
        # SHAP –∞–Ω–∞–ª–∏–∑
        shap_path = create_shap_plots(model, X_test, f"final_{model_type}")
        if shap_path:
            mlflow.log_artifact(shap_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        final_model_path = f"models/final_{model_type}_model.pkl"
        joblib.dump(model, final_model_path)
        mlflow.log_artifact(final_model_path)
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –≤ Model Registry
        if model_type in ["random_forest", "logistic_regression"]:
            model_info = mlflow.sklearn.log_model(
                model, 
                "model",
                registered_model_name=f"FlightDelay_{model_type.title()}"
            )
        elif model_type == "xgboost":
            model_info = mlflow.xgboost.log_model(
                model, 
                "model",
                registered_model_name=f"FlightDelay_{model_type.title()}"
            )
        
        print(f"\nFinal {model_type.upper()} Results:")
        print(f"Test F1-Score: {test_metrics['f1_score']:.4f}")
        print(f"Test Accuracy: {test_metrics['accuracy']:.4f}")
        print(f"Test ROC-AUC: {test_metrics.get('roc_auc', 'N/A')}")
        
        return model, test_metrics['f1_score'], model_info.model_uri

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="params.yaml", help="Config file path")
    parser.add_argument("--model", required=True,
                       choices=['random_forest', 'xgboost', 'logistic_regression'],
                       help="Model to train with best parameters")
    args = parser.parse_args()
    
    config = load_config(args.config)
    
    # Setup MLflow
    if config['experiments']['tracking_uri']:
        mlflow.set_tracking_uri(config['experiments']['tracking_uri'])
    
    experiment_name = config['experiments']['experiment_name']
    mlflow.set_experiment(experiment_name)
    
    # Load data
    X_train, X_test, y_train, y_test = load_data(config)
    
    # Load best parameters from hyperopt experiment
    hyperopt_experiment_name = f"{experiment_name}_hyperopt"
    best_params = load_best_params_from_mlflow(hyperopt_experiment_name, args.model)
    
    if best_params is None:
        print("No optimized parameters found. Using default parameters.")
        # Fallback to config parameters
        model_config = config['models'][args.model]
        best_params = {k: v[0] if isinstance(v, list) else v 
                      for k, v in model_config.items()}
    
    # Train final model
    model, score, model_uri = train_final_model(
        args.model, best_params, X_train, y_train, X_test, y_test
    )
    
    print(f"\nFinal model training completed!")
    print(f"Model URI: {model_uri}")
    print(f"F1-Score: {score:.4f}")

if __name__ == "__main__":
    main()

```


### –®–∞–≥ 10. –ó–∞–ø—É—Å–∫ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

```bash
# –û–±—É—á–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
python src/train_best_model.py --model random_forest
python src/train_best_model.py --model xgboost
python src/train_best_model.py --model logistic_regression

```

***

## üìä –ß–∞—Å—Ç—å 5. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å DVC

### –®–∞–≥ 11. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ DVC –ø–∞–π–ø–ª–∞–π–Ω–∞

–û–±–Ω–æ–≤–∏—Ç–µ dvc.yaml:

```text
stages:
  preprocess:
    cmd: python src/preprocess.py --config params.yaml
    deps:
      - data/raw/flights.csv
      - src/preprocess.py
      - params.yaml
    outs:
      - data/processed/flights_processed.csv
      - models/encoders.pkl
      - models/scaler.pkl
    params:
      - preprocess
      
  validate:
    cmd: python src/validate_data.py --data data/processed/flights_processed.csv
    deps:
      - data/processed/flights_processed.csv
      - src/validate_data.py
    outs:
      - reports/data_validation.json
      
  split:
    cmd: python src/split_data.py --config params.yaml
    deps:
      - data/processed/flights_processed.csv
      - src/split_data.py
      - params.yaml
    outs:
      - data/split/X_train.csv
      - data/split/X_test.csv
      - data/split/y_train.csv
      - data/split/y_test.csv
    params:
      - preprocess.test_size
      - preprocess.random_state

  train_baseline:
    cmd: python src/train.py --config params.yaml
    deps:
      - data/split/X_train.csv
      - data/split/X_test.csv
      - data/split/y_train.csv
      - data/split/y_test.csv
      - src/train.py
      - params.yaml
    outs:
      - models/random_forest_model.pkl
      - models/xgboost_model.pkl
      - models/logistic_regression_model.pkl
    metrics:
      - reports/baseline_metrics.json
    params:
      - models

```

### –®–∞–≥ 12. –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞ –¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫

–°–æ–∑–¥–∞–π—Ç–µ src/collect_metrics.py:

```python
import json
import mlflow
import yaml
import argparse
from datetime import datetime

def collect_baseline_metrics(config, output_path="reports/baseline_metrics.json"):
    """–°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ MLflow"""
    
    if config['experiments']['tracking_uri']:
        mlflow.set_tracking_uri(config['experiments']['tracking_uri'])
    
    experiment_name = config['experiments']['experiment_name']
    experiment = mlflow.get_experiment_by_name(experiment_name)
    
    if experiment is None:
        print(f"Experiment {experiment_name} not found")
        return
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–∞–ø—É—Å–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    metrics = {}
    
    for model_type in ['random_forest', 'xgboost', 'logistic_regression']:
        runs = mlflow.search_runs(
            experiment_ids=[experiment.experiment_id],
            filter_string=f"tags.model_type = '{model_type}' and tags.model_stage != 'final'",
            order_by=["start_time DESC"],
            max_results=1
        )
        
        if not runs.empty:
            run = runs.iloc[0]
            metrics[model_type] = {
                'test_f1_score': run.get(f'metrics.test_f1_score', None),
                'test_accuracy': run.get(f'metrics.test_accuracy', None),
                'test_roc_auc': run.get(f'metrics.test_roc_auc', None),
                'run_id': run['run_id']
            }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    import os
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump(metrics, f, indent=2)
    
    print(f"Baseline metrics saved to {output_path}")
    return metrics

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="params.yaml", help="Config file path")
    parser.add_argument("--output", default="reports/baseline_metrics.json", help="Output metrics file")
    args = parser.parse_args()
    
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    collect_baseline_metrics(config, args.output)

if __name__ == "__main__":
    main()

```

–û–±–Ω–æ–≤–∏—Ç–µ dvc.yaml, –¥–æ–±–∞–≤–∏–≤ —Å–±–æ—Ä –º–µ—Ç—Ä–∏–∫:

```text
  collect_metrics:
    cmd: python src/collect_metrics.py --config params.yaml --output reports/baseline_metrics.json
    deps:
      - src/collect_metrics.py
      - params.yaml
    outs:
      - reports/baseline_metrics.json
```

***

## üîç –ß–∞—Å—Ç—å 6. –ê–Ω–∞–ª–∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤

### –®–∞–≥ 13. –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞

```bash
# –ó–∞–ø—É—Å–∫–∞–µ–º –≤–µ—Å—å –ø–∞–π–ø–ª–∞–π–Ω
dvc repro

# –ü—Ä–æ–≤–µ—Ä—è–µ–º DAG
dvc dag

```


### –®–∞–≥ 14. –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤

–°–æ–∑–¥–∞–π—Ç–µ src/analyze_experiments.py:

```python
import mlflow
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import yaml
import argparse

def load_config(config_path='params.yaml'):
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def analyze_experiments(config):
    """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
    
    if config['experiments']['tracking_uri']:
        mlflow.set_tracking_uri(config['experiments']['tracking_uri'])
    
    experiment_name = config['experiments']['experiment_name']
    experiment = mlflow.get_experiment_by_name(experiment_name)
    
    if experiment is None:
        print(f"Experiment {experiment_name} not found")
        return
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∑–∞–ø—É—Å–∫–∏
    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
    
    if runs.empty:
        print("No runs found")
        return
    
    print(f"Found {len(runs)} runs")
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    metric_cols = [col for col in runs.columns if col.startswith('metrics.')]
    param_cols = [col for col in runs.columns if col.startswith('params.')]
    tag_cols = [col for col in runs.columns if col.startswith('tags.')]
    
    # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    summary_df = runs[['run_id', 'status', 'start_time'] + metric_cols + param_cols + tag_cols].copy()
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø—É –º–æ–¥–µ–ª–∏
    model_summary = []
    for model_type in ['random_forest', 'xgboost', 'logistic_regression']:
        model_runs = runs[runs['tags.model_type'] == model_type]
        if not model_runs.empty:
            best_run = model_runs.loc[model_runs['metrics.test_f1_score'].idxmax()]
            model_summary.append({
                'model_type': model_type,
                'best_f1_score': best_run['metrics.test_f1_score'],
                'best_accuracy': best_run['metrics.test_accuracy'],
                'best_roc_auc': best_run.get('metrics.test_roc_auc', None),
                'num_experiments': len(model_runs)
            })
    
    summary_df = pd.DataFrame(model_summary)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('ML Experiment Analysis', fontsize=16)
    
    # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ F1-score
    axes[0, 0].bar(summary_df['model_type'], summary_df['best_f1_score'])
    axes[0, 0].set_title('Best F1-Score by Model Type')
    axes[0, 0].set_ylabel('F1-Score')
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ Accuracy
    axes[0, 1].bar(summary_df['model_type'], summary_df['best_accuracy'], color='orange')
    axes[0, 1].set_title('Best Accuracy by Model Type')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    # 3. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
    axes[1, 0].bar(summary_df['model_type'], summary_df['num_experiments'], color='green')
    axes[1, 0].set_title('Number of Experiments by Model Type')
    axes[1, 0].set_ylabel('Number of Runs')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # 4. ROC-AUC —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    roc_data = summary_df.dropna(subset=['best_roc_auc'])
    if not roc_data.empty:
        axes[1, 1].bar(roc_data['model_type'], roc_data['best_roc_auc'], color='red')
        axes[1, 1].set_title('Best ROC-AUC by Model Type')
        axes[1, 1].set_ylabel('ROC-AUC')
        axes[1, 1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
    import os
    os.makedirs('reports', exist_ok=True)
    plt.savefig('reports/experiment_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # –ü–µ—á–∞—Ç–∞–µ–º —Å–≤–æ–¥–∫—É
    print("\n" + "="*60)
    print("EXPERIMENT SUMMARY")
    print("="*60)
    print(summary_df.to_string(index=False))
    
    return summary_df

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="params.yaml", help="Config file path")
    args = parser.parse_args()
    
    config = load_config(args.config)
    analyze_experiments(config)

if __name__ == "__main__":
    main()

```

### –®–∞–≥ 15. –ü–æ–ª–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞

```bash
# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
python src/analyze_experiments.py

# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –æ—Ç—á–µ—Ç—ã
ls -la reports/

```

***

## ‚úÖ –ß–∞—Å—Ç—å 7. –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

### –®–∞–≥ 16.  –ü—Ä–æ–≤–µ—Ä–∫–∞ Model Registry

```bash
# Flight Delay Prediction - Experiment Report

## –û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞
- **–ó–∞–¥–∞—á–∞**: –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–¥–µ—Ä–∂–µ–∫ —Ä–µ–π—Å–æ–≤
- **–î–∞–Ω–Ω—ã–µ**: –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∞–≤–∏–∞—Ä–µ–π—Å–æ–≤ (50,000 –∑–∞–ø–∏—Å–µ–π)
- **–¶–µ–ª–µ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞**: F1-score

## –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã

### Baseline –º–æ–¥–µ–ª–∏
| –ú–æ–¥–µ–ª—å | F1-Score | Accuracy | ROC-AUC | –ü–∞—Ä–∞–º–µ—Ç—Ä—ã |
|--------|----------|----------|---------|-----------|
| Random Forest | 0.XXXX | 0.XXXX | 0.XXXX | default |
| XGBoost | 0.XXXX | 0.XXXX | 0.XXXX | default |
| Logistic Regression | 0.XXXX | 0.XXXX | 0.XXXX | default |

### –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
| –ú–æ–¥–µ–ª—å | F1-Score | Accuracy | ROC-AUC | –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã |
|--------|----------|----------|---------|------------------|
| Random Forest | 0.XXXX | 0.XXXX | 0.XXXX | n_estimators=X, max_depth=Y |
| XGBoost | 0.XXXX | 0.XXXX | 0.XXXX | n_estimators=X, max_depth=Y |
| Logistic Regression | 0.XXXX | 0.XXXX | 0.XXXX | C=X |

## –í—ã–≤–æ–¥—ã
- –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: [MODEL_NAME]
- –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: [TOP_FEATURES]
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: [RECOMMENDATIONS]

## –§–∞–π–ª—ã
- MLflow tracking: http://localhost:5000
- –ú–æ–¥–µ–ª–∏: models/final_*_model.pkl
- –û—Ç—á–µ—Ç—ã: reports/
- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: params.yaml

```


### –®–∞–≥ 18. –ö–æ–º–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

```bash
# –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã
git add src/train.py src/hyperopt_tune.py src/train_best_model.py
git add src/collect_metrics.py src/analyze_experiments.py
git add params.yaml dvc.yaml reports/experiment_report.md

# –ö–æ–º–º–∏—Ç–∏–º
git commit -m "Add MLflow experiment tracking and model training

Features:
- Basic model training with MLflow logging
- Hyperparameter optimization with hyperopt
- Final model training with best parameters
- SHAP interpretability analysis
- Model Registry integration
- Experiment analysis and reporting

Models trained: Random Forest, XGBoost, Logistic Regression
Metrics: F1-score, Accuracy, ROC-AUC, Precision, Recall"

# –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º DVC
dvc push

```


### üìù –û—Ç—á–µ—Ç –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏

–ß—Ç–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω–æ:
1. –°–∫—Ä–∏–ø—Ç—ã –æ–±—É—á–µ–Ω–∏—è:

* src/train.py - –±–∞–∑–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
* src/hyperopt_tune.py - –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
* src/train_best_model.py - —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
* src/collect_metrics.py - —Å–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –¥–ª—è DVC
* src/analyze_experiments.py - –∞–Ω–∞–ª–∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤

2. MLflow –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã:

* –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –≤ flight_delay_prediction
* –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ flight_delay_prediction_hyperopt
* –ú–æ–¥–µ–ª–∏ –≤ Model Registry: FlightDelay_RandomForest, FlightDelay_Xgboost, FlightDelay_LogisticRegression

3. –û–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏:

* models/final_*_model.pkl - —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
* models/random_forest_model.pkl - –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏

4. –û—Ç—á–µ—Ç—ã –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏:

* reports/confusion_matrix_*.png - –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
* reports/feature_importance_*.png - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
* reports/shap_summary_*.png - SHAP –∞–Ω–∞–ª–∏–∑
* reports/experiment_analysis.png - —Å–≤–æ–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑
* reports/baseline_metrics.json - –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è DVC

**–ü—Ä–æ–≤–µ—Ä–æ—á–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:**

```bash
# 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ MLflow UI
curl http://localhost:5000

# 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
ls -la models/

# 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç—á–µ—Ç–æ–≤
ls -la reports/

# 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ DVC –ø–∞–π–ø–ª–∞–π–Ω–∞
dvc dag
dvc metrics show

# 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ Model Registry —á–µ—Ä–µ–∑ Python
python -c "import mlflow; print([m.name for m in mlflow.search_registered_models()])"

```


***


**–ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—à–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:**

‚úÖ MLflow UI –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ http://localhost:5000
‚úÖ –û–±—É—á–µ–Ω—ã –±–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –≤—Å–µ—Ö —Ç—Ä–µ—Ö —Ç–∏–ø–æ–≤ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≤ MLflow
‚úÖ –ü—Ä–æ–≤–µ–¥–µ–Ω–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
‚úÖ –û–±—É—á–µ–Ω—ã —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ Model Registry
‚úÖ –°–æ–∑–¥–∞–Ω—ã –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (confusion matrix, feature importance, SHAP)
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–æ —Å DVC –ø–∞–π–ø–ª–∞–π–Ω–æ–º
‚úÖ –í–æ–∑–º–æ–∂–Ω–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –≤ MLflow UI

–≠—Ç–∞ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è ML-–º–æ–¥–µ–ª–µ–π —Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –≤ MLflow.

